# Fullstack GPT

ë­ì²´ì¸ìœ¼ë¡œ AI ì›¹ ì„œë¹„ìŠ¤ 7ê°œ ë§Œë“¤ê¸°

| í”„ë¡œì íŠ¸ ê¸°ê°„ | 23.11.11 ~                                                                                                                              |
| ------------- | --------------------------------------------------------------------------------------------------------------------------------------- |
| í”„ë¡œì íŠ¸ ëª©ì  | Langchain, Language Models ì— ëŒ€í•œ ê¸°ë³¸ ì´í•´, ìì²´ ë°ì´í„°ì— GPT-4ë¥¼ ì‚¬ìš©í•˜ëŠ” ë°©ë²•, ì»¤ìŠ¤í…€ ììœ¨ ì—ì´ì „íŠ¸(Autonomous Agent)ë¥¼ ë§Œë“œëŠ” ë°©ë²• |
| Github        | https://github.com/Jinwook-Song/fullstack-gpt                                                                                           |

## TODOS

- AI ì›¹ ì„œë¹„ìŠ¤ (6ì¢…) : DocumentGPT, PrivateGPT, QuizGPT, SiteGPT, MeetingGPT, InvestorGPT
- ChatGPT í”ŒëŸ¬ê·¸ì¸ (1ì¢…) : ChefGPT
- í™œìš©í•˜ëŠ” íŒ¨í‚¤ì§€ : Langchain, GPT-4, Whisper, FastAPI, Streamlit, Pinecone, Hugging Face, ...

### DocumentGPT

ë²•ë¥ . ì˜í•™ ë“± ì–´ë ¤ìš´ ìš©ì–´ë¡œ ê°€ë“í•œ ê°ì¢… ë¬¸ì„œ. AIë¡œ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ê³  ì‹¶ë‹¤ë©´?

AIë¡œ ì‹ ì†í•˜ê³  ì •í™•í•˜ê²Œ ë¬¸ì„œ ë‚´ìš©ì„ íŒŒì•…í•˜ê³  ì •ë¦¬í•œ ë’¤, í•„ìš”í•œ ë¶€ë¶„ë§Œ ì™ì™ ê³¨ë¼ë‚´ì–´ ì‚¬ìš©í•˜ì„¸ìš”. DocumentGPT ì±—ë´‡ì„ ì‚¬ìš©í•˜ë©´, AIê°€ ë¬¸ì„œ(.txt, .pdf, .docx ë“±)ë¥¼ ê¼¼ê¼¼í•˜ê²Œ ì½ê³ , í•´ë‹¹ ë¬¸ì„œì— ê´€í•œ ì§ˆë¬¸ì— ì²™ì²™ ë‹µë³€í•´ ì¤ë‹ˆë‹¤.

### PrivateGPT

íšŒì‚¬ ê¸°ë°€ì´ ìœ ì¶œë ê¹Œ ê±±ì •ëœë‹¤ë©´? ì´ì œ ë‚˜ë§Œì´ ë³¼ ìˆ˜ ìˆëŠ” ë¹„ê³µê°œ GPTë¥¼ ë§Œë“¤ì–´ í™œìš©í•˜ì„¸ìš”!

DocumentGPTì™€ ë¹„ìŠ·í•˜ì§€ë§Œ ë¡œì»¬ ì–¸ì–´ ëª¨ë¸ì„ ì‚¬ìš©í•´ ë¹„ê³µê°œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ê¸°ì— ì í•©í•œ ì±—ë´‡ì…ë‹ˆë‹¤. ë°ì´í„°ëŠ” ì»´í“¨í„°ì— ë³´ê´€ë˜ë¯€ë¡œ ì˜¤í”„ë¼ì¸ì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì¶œ ê±±ì • ì—†ì´ í•„ìš”í•œ ë°ì´í„°ë¥¼ PrivateGPTì— ë§¡ê¸°ê³  ì—…ë¬´ ìƒì‚°ì„±ì„ ë†’ì¼ ìˆ˜ ìˆì–´ìš”.

### QuizGPT

ì•”ê¸°í•´ì•¼ í•  ë‚´ìš©ì„ íš¨ìœ¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ì‹¶ë‹¤ë©´?

ë¬¸ì„œë‚˜ ìœ„í‚¤í”¼ë””ì•„ ë“± í•™ìŠµì´ í•„ìš”í•œ ì»¨í…ì¸ ë¥¼ AIì—ê²Œ í•™ìŠµì‹œí‚¤ë©´, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í€´ì¦ˆë¥¼ ìƒì„±í•´ ì£¼ëŠ” ì•±ì…ë‹ˆë‹¤. ë²ˆê±°ë¡œìš´ ê³¼ì •ì„ ìµœì†Œí™”í•˜ê³  í•™ìŠµ íš¨ìœ¨ì„ ê·¹ëŒ€í™”í•  ìˆ˜ ìˆì–´, íŠ¹íˆ ì‹œí—˜ì´ë‚˜ ë‹¨ê¸°ê°„ ê³ íš¨ìœ¨ í•™ìŠµì´ í•„ìš”í•  ë•Œ ë§¤ìš° ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆì–´ìš”.

### SiteGPT

ìì£¼ ë¬»ëŠ” ì§ˆë¬¸ ë•Œë¬¸ì— CS ì§ì›ì„ ì±„ìš©...? SiteGPTë¡œ ë¹„ìš©ì„ 2ë°° ì ˆê°í•´ ë´…ì‹œë‹¤.

ì›¹ì‚¬ì´íŠ¸ë¥¼ ìŠ¤í¬ë©í•˜ì—¬ ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ê³ , í•´ë‹¹ ì¶œì²˜ë¥¼ ì¸ìš©í•˜ì—¬ ê´€ë ¨ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤. ê³ ê° ì‘ëŒ€ì˜ ëŒ€ë¶€ë¶„ì„ ì°¨ì§€í•˜ëŠ” ë‹¨ìˆœ ì •ë³´ ì•ˆë‚´ì— ë“¤ì´ëŠ” ì‹œê°„ì„ íšê¸°ì ìœ¼ë¡œ ì¤„ì¼ ìˆ˜ ìˆê³ , ê³ ê° ë˜í•œ CSì§ì›ì˜ ê·¼ë¬´ ì‹œê°„ì— êµ¬ì• ë°›ì§€ ì•Šê³  ì •í™•í•œ ì •ë³´ë¥¼ ë¹ ë¥´ê²Œ ì „ë‹¬ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### MeetingGPT

ì´ì œ íšŒì˜ë¡ ì •ë¦¬ëŠ” MeetingGPTì—ê²Œ ë§¡ê¸°ì„¸ìš”!

íšŒì˜ ì˜ìƒ ë‚´ìš©ì„ í† ëŒ€ë¡œ ì˜¤ë””ì˜¤ ì¶”ì¶œ, ì½˜í…ì¸ ë¥¼ ìˆ˜ì§‘í•˜ì—¬ íšŒì˜ë¡ì„ ìš”ì•½ ë° ì‘ì„±í•´ ì£¼ëŠ” ì•±ì…ë‹ˆë‹¤. íšŒì˜ ë‚´ìš©ì„ ê¸°ë¡í•˜ëŠë¼ íšŒì˜ì— ì œëŒ€ë¡œ ì°¸ì„í•˜ì§€ ëª»í•˜ëŠ” ì¼ì„ ë°©ì§€í•  ìˆ˜ ìˆê³ , ê´€ë ¨ ì§ˆì˜ì‘ë‹µë„ ê°€ëŠ¥í•´ ë‹¨ìˆœí•œ ê¸°ë¡ë³´ë‹¤ í›¨ì”¬ ë” íš¨ìœ¨ì ìœ¼ë¡œ íšŒì˜ë¡ì„ ê´€ë¦¬í•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### InvestorGPT

AIê°€ ìë£Œ ì¡°ì‚¬ë„ ì•Œì•„ì„œ ì²™ì²™ í•´ ì¤ë‹ˆë‹¤.

ì¸í„°ë„·ì„ ê²€ìƒ‰í•˜ê³  íƒ€ì‚¬ APIë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ììœ¨ ì—ì´ì „íŠ¸ì…ë‹ˆë‹¤. íšŒì‚¬, ì£¼ê°€ ë° ì¬ë¬´ì œí‘œë¥¼ ì¡°ì‚¬í•˜ì—¬ ì¬ë¬´ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ì•Œì•„ì„œ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ìˆ˜ì§‘í•˜ê¸° ë•Œë¬¸ì— ì§ì ‘ SQL ì¿¼ë¦¬ë¥¼ ì‘ì„±í•  í•„ìš”ê°€ ì—†ê³ , í•´ë‹¹ ë‚´ìš©ì— ëŒ€í•œ ì§ˆì˜ì‘ë‹µë„ ì–¼ë§ˆë“ ì§€ ê°€ëŠ¥í•©ë‹ˆë‹¤.

### ChefGPT

ìš”ì¦˜ í•«í•œ ChatGPT í”ŒëŸ¬ê·¸ì¸? ì§ì ‘ êµ¬í˜„í•´ ë´ìš”!

ìœ ì €ê°€ ChatGPT í”ŒëŸ¬ê·¸ì¸ ìŠ¤í† ì–´ì—ì„œ ì„¤ì¹˜í•  ìˆ˜ ìˆëŠ” ChatGPT í”ŒëŸ¬ê·¸ì¸ì…ë‹ˆë‹¤. ì´ í”ŒëŸ¬ê·¸ì¸ì„ í†µí•´ ìœ ì €ëŠ” ChatGPT ì¸í„°í˜ì´ìŠ¤ì—ì„œ ë°”ë¡œ ë ˆì‹œí”¼ë¥¼ ê²€ìƒ‰í•˜ê³  ì¡°ë¦¬ë²•ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ ChatGPT í”ŒëŸ¬ê·¸ì¸ì—ì„œ OAuth ì¸ì¦ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œë„ ë°°ì›ë‹ˆë‹¤.

- prompt & template

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate

chat = ChatOpenAI(temperature=0.1)  # randomness of response

template = PromptTemplate.from_template(
    "What is the distance between {country_a} and {country_b}",
)

prompt = template.format(country_a="Mexico", country_b="Korea")

chat.predict(prompt)

template = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a geography expert. And you only reply in {language}."),
        ("ai", "Ciao, mi chiamo {name}!"),
        (
            "human",
            "What is the distance between {country_a} and {country_b}. Also, what is your name?",
        ),
    ]
)

prompt = template.format_messages(
    language="Greek", name="Socrates", country_a="Mexico", country_b="Korea"
)

chat.predict_messages(prompt)
```

## LangChain

- prompt & template

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate

chat = ChatOpenAI(temperature=0.1)  # randomness of response

template = PromptTemplate.from_template(
    "What is the distance between {country_a} and {country_b}",
)

prompt = template.format(
    country_a="Mexico",
    country_b="Korea",
)

chat.predict(prompt)
```

```python
template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a geography expert. And you only reply in {language}.",
        ),
        (
            "ai",
            "Ciao, mi chiamo {name}!",
        ),
        (
            "human",
            "What is the distance between {country_a} and {country_b}. Also, what is your name?",
        ),
    ]
)

prompt = template.format_messages(
    language="Greek", name="Socrates", country_a="Mexico", country_b="Korea"
)

chat.predict_messages(prompt)
```

- parser

```python
from langchain.schema import BaseOutputParser

class CommaOutputParser(BaseOutputParser):
    def parse(self, text):
        items = text.strip().split(",")
        return list(map(str.strip, items))
```

```python
template = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a list geterating machine.Everything you are asked will be answered with a comma seperated list of max {max_items} in lowercase. Do NOT reply with anything else.",
        ),
        (
            "human",
            "{question}",
        ),
    ]
)

prompt = template.format_messages(max_items=10, question="What are the planets?")

result = chat.predict_messages(prompt)

p = CommaOutputParser()

p.parse(result.content)
```

- chain [docs](https://python.langchain.com/docs/expression_language/interface)
  - ìœ„ì˜ ê³¼ì •ì„ chainìœ¼ë¡œ ë‹¨ìˆœí™” í•  ìˆ˜ ìˆë‹¤.

```python
chain = template | chat | CommaOutputParser()

chain.invoke({"max_items": 5, "question": "What are the pokemons?"})
```

- chaing chains

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate, ChatPromptTemplate

chat = ChatOpenAI(temperature=0.1)  # randomness of response

chef_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a world-calss international chef. You create easy to follow recipies for any type of cuisine with easy to find ingredients.",
        ),
        ("human", "I want to cook {cuisine} food."),
    ]
)

chef_chain = chef_prompt | chat

veg_chef_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You are a vegetarian chef specialized on making traditional recipies vegetarian. You find alternative ingredients and explain their preparation. You don't radically modify the recipe. If there is no alternative for a food just say you don't know how to replace it.",
        ),
        ("human", "{recipe}"),
    ]
)

veg_chain = veg_chef_prompt | chat

final_chain = {"recipe": chef_chain} | veg_chain

final_chain.invoke({"cuisine": "indian"})
```

## Model IO ([docs](https://python.langchain.com/docs/modules/))

- \***\*FewShotPromptTemplate\*\***
  - íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•  ìˆ˜ ìˆë„ë¡ í•´ì¤Œ

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain.prompts.few_shot import FewShotPromptTemplate
from langchain.callbacks import StreamingStdOutCallbackHandler

chat = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

examples = [
    {
        "question": "What do you know about France?",
        "answer": """
        Here is what I know:
        Capital: Paris
        Language: French
        Food: Wine and Cheese
        Currency: Euro
        """,
    },
    {
        "question": "What do you know about Italy?",
        "answer": """
        I know this:
        Capital: Rome
        Language: Italian
        Food: Pizza and Pasta
        Currency: Euro
        """,
    },
    {
        "question": "What do you know about Greece?",
        "answer": """
        I know this:
        Capital: Athens
        Language: Greek
        Food: Souvlaki and Feta Cheese
        Currency: Euro
        """,
    },
]

example_template = """
    Human: {question}
    AI: {answer}
"""

example_prompt = PromptTemplate.from_template(example_template)

prompt = FewShotPromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
    suffix="Human: What do you know about {country}?",
    input_variables=["country"],
)

chain = prompt | chat

chain.invoke({"country": "Germany"})
```

- FewShotChatMessagePromptTemplate

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.prompts.few_shot import FewShotChatMessagePromptTemplate
from langchain.callbacks import StreamingStdOutCallbackHandler

chat = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

examples = [
    {
        "country": "France",
        "answer": """
        Here is what I know:
        Capital: Paris
        Language: French
        Food: Wine and Cheese
        Currency: Euro
        """,
    },
    {
        "country": "Italy",
        "answer": """
        I know this:
        Capital: Rome
        Language: Italian
        Food: Pizza and Pasta
        Currency: Euro
        """,
    },
    {
        "country": "Greece",
        "answer": """
        I know this:
        Capital: Athens
        Language: Greek
        Food: Souvlaki and Feta Cheese
        Currency: Euro
        """,
    },
]

# only to format examples
example_prompt = ChatPromptTemplate.from_messages(
    [
        ("human", "What do you know about {country}?"),
        ("ai", "{answer}"),
    ]
)

example_prompt = FewShotChatMessagePromptTemplate(
    example_prompt=example_prompt,
    examples=examples,
)

final_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a geography expert, you give short answers."),
        example_prompt,
        ("human", "What do you know about {country}?"),
    ]
)

chain = final_prompt | chat

chain.invoke({"country": "Korea"})
```

- \***\*LengthBasedExampleSelector\*\***

```python
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain.prompts.few_shot import (
    FewShotPromptTemplate,
    FewShotChatMessagePromptTemplate,
)
from langchain.prompts.example_selector import LengthBasedExampleSelector

chat = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

examples = [
    {
        "question": "What do you know about France?",
        "answer": """
        Here is what I know:
        Capital: Paris
        Language: French
        Food: Wine and Cheese
        Currency: Euro
        """,
    },
    {
        "question": "What do you know about Italy?",
        "answer": """
        I know this:
        Capital: Rome
        Language: Italian
        Food: Pizza and Pasta
        Currency: Euro
        """,
    },
    {
        "question": "What do you know about Greece?",
        "answer": """
        I know this:
        Capital: Athens
        Language: Greek
        Food: Souvlaki and Feta Cheese
        Currency: Euro
        """,
    },
]

example_prompt = PromptTemplate.from_template("Human: {question}\nAI:{answer}")

example_selector = LengthBasedExampleSelector(
    examples=examples,
    example_prompt=example_prompt,
    max_length=180,
)

prompt = FewShotPromptTemplate(
    example_prompt=example_prompt,
    example_selector=example_selector,
    suffix="Human: What do you know about {country}?",
    input_variables=["country"],
)

prompt.format(country="Brazil")
```

- Custom Selector
  - selectorë¥¼ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤.
  - `add_example`, `select_examples` methodë¥¼ êµ¬í˜„í•´ì•¼ í•œë‹¤

```python
class RandomExampleSelector(BaseExampleSelector):
    def __init__(self, examples):
        self.examples = examples

    def add_example(self, example):
        self.examples.append(example)

    def select_examples(self, input_variables):
        from random import choice

        return [choice(self.examples)]

example_prompt = PromptTemplate.from_template("Human: {question}\nAI:{answer}")

example_selector = RandomExampleSelector(examples=examples)

prompt = FewShotPromptTemplate(
    example_prompt=example_prompt,
    example_selector=example_selector,
    suffix="Human: What do you know about {country}?",
    input_variables=["country"],
)

prompt.format(country="Brazil")
```

- Serialization with PipelinePromptTemplate

```python
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate
from langchain.prompts.pipeline import PipelinePromptTemplate

chat = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

intro = PromptTemplate.from_template(
    """
    You are a role playing assistant.
    And you are impersonating a {character}
    """
)

example = PromptTemplate.from_template(
    """
    This is an example of how you talk:
    Human: {example_question}
    You: {example_answer}
    """
)

start = PromptTemplate.from_template(
    """
    Start now!

    Human: {question}
    You:
    """
)

final = PromptTemplate.from_template(
    """
    {intro}

    {example}

    {start}
    """
)

pipeline_prompts = [
    ("intro", intro),
    ("example", example),
    ("start", start),
]

full_prompt = PipelinePromptTemplate(
    pipeline_prompts=pipeline_prompts,
    final_prompt=final,
)

chain = full_prompt | chat

chain.invoke(
    {
        "character": "Pirate",
        "example_question": "What is your location?",
        "example_answer": "Arrrrg! That is a secret!! Arg arg!",
        "question": "What is your favorite food?",
    }
)
```

- caching

```python
from langchain.chat_models import ChatOpenAI
from langchain.callbacks import StreamingStdOutCallbackHandler
from langchain.prompts import PromptTemplate
from langchain.prompts.pipeline import PipelinePromptTemplate
from langchain.globals import set_llm_cache, set_debug
from langchain.cache import InMemoryCache, SQLiteCache

set_llm_cache(SQLiteCache("cache.db"))
set_debug(True)

chat = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[StreamingStdOutCallbackHandler()],
)

chat.predict("How do you make italian pasta")
```

## Memory ([docs](https://python.langchain.com/docs/modules/memory/))

- ConversationBufferMemory

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory(return_messages=True)

memory.save_context(
    {"input": "Hi"},
    {"output": "How are you?"},
)

memory.load_memory_variables({})
```

- ConversationBufferWindowMemory
  ì €ì¥í•  ë²”ìœ„ ì§€ì •

```python
from langchain.memory import ConversationBufferWindowMemory

memory = ConversationBufferWindowMemory(
    return_messages=True,
    k=4,
)
```

- ConversationSummaryMemory

```python
from langchain.memory import ConversationSummaryMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryMemory(llm=llm)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})

def get_history():
    return memory.load_memory_variables({})

add_message(
    "Hi I'm Jinwook, I live in South Korea",
    "Wow that is so cool!",
)
```

- ConversationSummaryBufferMemory
  token limit ì´ì „ messageì— ëŒ€í•´ì„œ summary

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=100,
    return_messages=True,
)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})

def get_history():
    return memory.load_memory_variables({})

add_message("Hi I'm Jinwook, I live in South Korea", "Wow that is so cool!")
```

- ConversationKGMemory

```python
from langchain.memory import ConversationKGMemory
from langchain.chat_models import ChatOpenAI

llm = ChatOpenAI(temperature=0.1)

memory = ConversationKGMemory(
    llm=llm,
    return_messages=True,
)

def add_message(input, output):
    memory.save_context({"input": input}, {"output": output})

add_message("Hi I'm Jinwook, I live in South Korea", "Wow that is so cool!")
```

- \***\*Memory on LLMChain\*\***
  chat historyì— ëŒ€í•œ ì •ë³´ë¥¼ ë„£ì–´ì¤€ë‹¤

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=120,
    memory_key="chat_history",
)

template = """
    You are a helpful AI talking to a human.

    {chat_history}
    Human:{question}
    You:
"""

chain = LLMChain(
    llm=llm,
    memory=memory,
    prompt=PromptTemplate.from_template(template),
    verbose=True,
)

chain.predict(question="My name is jinwook")
```

- Chat based memory

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.chains import LLMChain
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=120,
    memory_key="chat_history",
    return_messages=True,
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful AI talking to a human"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)

chain = LLMChain(
    llm=llm,
    memory=memory,
    prompt=prompt,
    verbose=True,
)

chain.predict(question="My name is jinwook")
```

- \***\*LCEL Based Memory\*\***

```python
from langchain.memory import ConversationSummaryBufferMemory
from langchain.chat_models import ChatOpenAI
from langchain.schema.runnable import RunnablePassthrough
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder

llm = ChatOpenAI(temperature=0.1)

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=120,
    memory_key="chat_history",
    return_messages=True,
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", "You are a helpful AI talking to a human"),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{question}"),
    ]
)

def load_memory(_):
    return memory.load_memory_variables({})["chat_history"]

def invoke_chain(question):
    result = chain.invoke({"question": question})
    memory.save_context(
        {"input": question},
        {"output": result.content},
    )
    print(result)

chain = RunnablePassthrough.assign(chat_history=load_memory) | prompt | llm

invoke_chain("My name is jinwook")
```

## RAG (Retrieval-Augmented Generation)

- ë‹¤ì–‘í•œ í¬ë§·ì˜ ë¬¸ì„œ load â†’ split (ë¹„ìš© ì ˆê° ë° LLMì˜ ì„±ëŠ¥ì„ ìœ„í•´) â†’ embedding (textë¥¼ ì˜ë¯¸ë³„ë¡œ ë²¡í„°í™” + caching) â†’ RetrivalQA chain ìƒì„± (ë‹¤ì–‘í•œ chain type)

```
"RAG"ëŠ” "Retrieval-Augmented Generation"ì˜ ì•½ìë¡œ, "ê²€ìƒ‰-ì¦ê°• ìƒì„±"ì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ìì—°ì–´ ì²˜ë¦¬(NLP) ë° ê¸°ê³„ í•™ìŠµ ë¶„ì•¼, íŠ¹íˆ ì±—ë´‡ì´ë‚˜ ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œê³¼ ê°™ì€ ê³ ê¸‰ ì–¸ì–´ ëª¨ë¸ì„ êµ¬ì¶•í•˜ëŠ” ë° ì‚¬ìš©ë˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.

RAGì— ëŒ€í•œ ê°„ëµí•œ ê°œìš”ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

ê²€ìƒ‰ê³¼ ìƒì„±ì˜ ê²°í•©: RAGëŠ” NLPì˜ ë‘ ê°€ì§€ ì£¼ìš” êµ¬ì„± ìš”ì†Œì¸ ì •ë³´ ê²€ìƒ‰ê³¼ ì‘ë‹µ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤. ê²€ìƒ‰ ë¶€ë¶„ì€ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ê¸° ìœ„í•´ ëŒ€ê·œëª¨ ë°ì´í„°ë² ì´ìŠ¤ë‚˜ ë¬¸ì„œ ì»¬ë ‰ì…˜ì„ ê²€ìƒ‰í•˜ëŠ” ê³¼ì •ì„ í¬í•¨í•©ë‹ˆë‹¤. ìƒì„± ë¶€ë¶„ì€ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¼ê´€ë˜ê³  ë§¥ë½ì ìœ¼ë¡œ ì ì ˆí•œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤.
ì‘ë™ ë°©ì‹: RAG ì‹œìŠ¤í…œì—ì„œ ì§ˆë¬¸ì´ë‚˜ í”„ë¡¬í”„íŠ¸ê°€ ì£¼ì–´ì§€ë©´ ëª¨ë¸ì€ ë¨¼ì € ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ì œê³µí•˜ëŠ” ë° ìœ ìš©í•œ ì •ë³´ë¥¼ í¬í•¨í•  ìˆ˜ ìˆëŠ” ê´€ë ¨ ë¬¸ì„œë‚˜ í…ìŠ¤íŠ¸ë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ ê²€ìƒ‰ëœ ì •ë³´ë¥¼ ìƒì„± ëª¨ë¸ì— ê³µê¸‰í•˜ì—¬ ì¼ê´€ëœ ì‘ë‹µì„ í•©ì„±í•©ë‹ˆë‹¤.
ì¥ì : RAGì˜ ì£¼ìš” ì¥ì ì€ ëª¨ë¸ì´ ì™¸ë¶€ ì§€ì‹ì„ í™œìš©í•  ìˆ˜ ìˆê²Œ í•˜ì—¬ ë³´ë‹¤ ì •í™•í•˜ê³  ìƒì„¸í•˜ë©° ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ëœ ë‹µë³€ì„ ì œê³µí•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ëŠ” íŠ¹ì • ì§€ì‹ì´ë‚˜ ì‚¬ì‹¤ì  ì •ë³´ê°€ í•„ìš”í•œ ì§ˆë¬¸ì— íŠ¹íˆ ìœ ìš©í•©ë‹ˆë‹¤.
ì‘ìš© ë¶„ì•¼: RAGëŠ” ì±—ë´‡, ì§ˆë¬¸-ì‘ë‹µ ì‹œìŠ¤í…œ ë° ì •í™•í•˜ê³  ë§¥ë½ì ìœ¼ë¡œ ê´€ë ¨ëœ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ë‹¤ë¥¸ AI ë„êµ¬ì™€ ê°™ì€ ë‹¤ì–‘í•œ ì‘ìš© ë¶„ì•¼ì— ì‚¬ìš©ë©ë‹ˆë‹¤. íŠ¹íˆ ëª¨ë¸ì´ ë‹¤ì–‘í•œ ì£¼ì œì™€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì´í•´í•˜ê³  ì‘ë‹µì„ ìƒì„±í•´ì•¼ í•˜ëŠ” ìƒí™©ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤.
ê°œë°œ ë° ì‚¬ìš©: AI ë° ê¸°ê³„ í•™ìŠµ ì»¤ë®¤ë‹ˆí‹°ì—ì„œ RAGëŠ” ë‹¤ì–‘í•œ ì—°êµ¬ ë…¼ë¬¸ê³¼ êµ¬í˜„ì´ ê°œë°œë˜ê³  ìˆìœ¼ë©° ì£¼ìš” ì´ˆì  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµëœ ì •ë³´ë¿ë§Œ ì•„ë‹ˆë¼ ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ìƒˆë¡­ê³  ê´€ë ¨ëœ ì •ë³´ë¥¼ í†µí•©í•˜ì—¬ ì‘ë‹µì˜ ì§ˆê³¼ ê´€ë ¨ì„±ì„ í–¥ìƒì‹œí‚¤ëŠ” ë” ì •êµí•œ AI ì‹œìŠ¤í…œìœ¼ë¡œ ë‚˜ì•„ê°€ëŠ” ë‹¨ê³„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.
```

[docs](https://python.langchain.com/docs/modules/chains/document/)

- load and split document

  - UnstructuredFileLoader ì„ì˜ì˜ í¬ë§· ë¬¸ì„œë¥¼ load í•  ìˆ˜ ìˆë‹¤
  - RecursiveCharacterTextSplitter
  - CharacterTextSplitter (seperatorë¥¼ ì§€ì •í•  ìˆ˜ ìˆë‹¤)
    - chunk_overlap ì´ì „ì˜ textë¥¼ ì–´ëŠì •ë„ë¡œ ê²¹ì¹˜ê²Œ ë‚˜ëˆŒì§€
    - from_tiktoken_encoder: tokenì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ í…ìŠ¤íŠ¸ë¥¼ ì„¸ëŠ”ê²ƒê³¼ ë™ì¼í•˜ê²Œ

  ```python
  from langchain.document_loaders import UnstructuredFileLoader
  from langchain.text_splitter import CharacterTextSplitter

  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
      separator="\n",
      chunk_size=600,
      chunk_overlap=100,
  )

  loader = UnstructuredFileLoader("./files/chapter_one.docx")

  docs = loader.load_and_split(text_splitter=text_splitter)

  len(docs)
  ```

- Embedding
  textë¥¼ vectorë¡œ ë³€í™˜

```python
from langchain.embeddings import OpenAIEmbeddings

embedder = OpenAIEmbeddings()

# get vertor for the text
vector = embedder.embed_documents(texts=["hello", "how", "are", "you"])

vector
```

```python
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores.chroma import Chroma

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=600,
    chunk_overlap=100,
)

loader = UnstructuredFileLoader("./files/chapter_one.docx")

documents = loader.load_and_split(text_splitter=text_splitter)

vectorstore = Chroma.from_documents(documents=documents, embedding=OpenAIEmbeddings())
```

- RetrievalQA
  `stuff`
  : This chain takes a list of documents and formats them all into a prompt, then passes that prompt to an LLM. It passes ALL documents, so you should make sure it fits within the context window the LLM you are using.
  `refine`
  : This chain collapses documents by generating an initial answer based on the first document and then looping over the remaining documents toÂ *refine*Â its answer. This operates sequentially, so it cannot be parallelized. It is useful in similar situatations as MapReduceDocuments Chain, but for cases where you want to build up an answer by refining the previous answer (rather than parallelizing calls).
  `map-reduce`
  : This chain first passes each document through an LLM, then reduces them using the ReduceDocumentsChain. Useful in the same situations as ReduceDocumentsChain, but does an initial LLM call before trying to reduce the documents.
  `map-rerank`
  : This calls on LLM on each document, asking it to not only answer but also produce a score of how confident it is. The answer with the highest confidence is then returned. This is useful when you have a lot of documents, but only want to answer based on a single document, rather than trying to combine answers (like Refine and Reduce methods do).

```python
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
from langchain.vectorstores.chroma import Chroma
from langchain.vectorstores.faiss import FAISS
from langchain.chains import RetrievalQA
from langchain.storage import LocalFileStore

cache_dir = LocalFileStore("./.cache/")

llm = ChatOpenAI()

text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
    separator="\n",
    chunk_size=600,
    chunk_overlap=100,
)

loader = UnstructuredFileLoader("./files/chapter_one.docx")

documents = loader.load_and_split(text_splitter=text_splitter)

embeddings = OpenAIEmbeddings()

cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
    embeddings,
    cache_dir,
)

vectorstore = FAISS.from_documents(
    documents=documents,
    embedding=cached_embeddings,
)

chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever(),
)

# chain.run("Where does Winston live?")
chain.run("Describe Victory Mansions")
```

- Stuff LCEL Chain
  | Component | Input Type | Output Type |
  | ------------ | ----------------------------------------------------- | --------------------- |
  | Prompt | Dictionary | PromptValue |
  | ChatModel | Single string, list of chat messages or a PromptValue | ChatMessage |
  | LLM | Single string, list of chat messages or a PromptValue | String |
  | OutputParser | The output of an LLM or ChatModel | Depends on the parser |
  | Retriever | Single string | List of Documents |
  | Tool | Single string or dictionary, depending on the tool | Depends on the tool |
  ì˜ˆì‹œì—ì„œ `Describe Victory Mansions`ë¥¼ inputìœ¼ë¡œ ë°›ì•„ Document listë¥¼ ë°˜í™˜

  ```python
  from langchain.chat_models import ChatOpenAI
  from langchain.storage import LocalFileStore
  from langchain.document_loaders import UnstructuredFileLoader
  from langchain.text_splitter import CharacterTextSplitter
  from langchain.embeddings import OpenAIEmbeddings, CacheBackedEmbeddings
  from langchain.vectorstores.faiss import FAISS
  from langchain.prompts import ChatPromptTemplate
  from langchain.schema.runnable import RunnablePassthrough

  cache_dir = LocalFileStore("./.cache/")

  llm = ChatOpenAI(temperature=0.1)

  text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
      separator="\n",
      chunk_size=600,
      chunk_overlap=100,
  )

  loader = UnstructuredFileLoader("./files/chapter_one.docx")

  documents = loader.load_and_split(text_splitter=text_splitter)

  embeddings = OpenAIEmbeddings()

  cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
      embeddings,
      cache_dir,
  )

  vectorstore = FAISS.from_documents(
      documents=documents,
      embedding=cached_embeddings,
  )

  retriver = vectorstore.as_retriever()

  prompt = ChatPromptTemplate.from_messages(
      [
          (
              "system",
              """
  You are a helpful assistant. Answer questions using only the context. If you don't know the answer, just say you don't know, don't make it up:\n\n{context}
  """,
          ),
          ("human", "{question}"),
      ]
  )

  chain = {"context": retriver, "question": RunnablePassthrough()} | prompt | llm

  chain.invoke("Describe Victory Mansions")
  ```

- \***\*Map Reduce LCEL Chain\*\***

  ```python
  map_doc_prompt = ChatPromptTemplate.from_messages(
      [
          (
              "system",
              """
              Use the following portion of a long document to see if any of the text is relevant to answer the question. Return any relevant text verbatim. If there is no relevant text, return : ''
              -------
              {context}
              """,
          ),
          ("human", "{question}"),
      ]
  )

  map_doc_chain = map_doc_prompt | llm

  def map_docs(inputs):
      documents = inputs["documents"]
      question = inputs["question"]
      results = []
      for document in documents:
          result = map_doc_chain.invoke(
              {"context": document.page_content, "question": question}
          ).content
          results.append(result)
      results = "\n\n".join(results)
      return results

  map_chain = {
      "documents": retriever,
      "question": RunnablePassthrough(),
  } | RunnableLambda(map_docs)

  final_prompt = ChatPromptTemplate.from_messages(
      [
          (
              "system",
              """
  Given the following extracted parts of a long document and a question, create a final answer.
  If you don't know the answer, just say that you don't know. Don't try to make up an answer.
  -----
  {context}
  """,
          ),
          ("human", "{question}"),
      ]
  )

  chain = {"context": map_chain, "question": RunnablePassthrough()} | final_prompt | llm

  # chain.invoke("Describe Victory Mansions")
  chain.invoke("Where dos Winston go to work?")
  ```

## Streamlit

`streamlit run Home.py`

```python
import streamlit as st

st.title("Hello world!")

st.subheader('Welcome to streamlit')

st.markdown("""
            ### I love it
            """)

today = datetime.today().strftime("%H:%M:%S")
st.title(today)

model = st.selectbox(
    label="Choose model",
    options=(
        "GPT-3.5",
        "GPT-4",
    ),
)
if model == "GPT-3.5":
    st.write("cheap")
else:
    st.write("expensive")

name = st.text_input("What's your name?")
st.write(name)

value = st.slider(
    label="temperature",
    min_value=-5,
    max_value=40,
    step=5,
)
st.write(value)
```

## Document GPT

```python
import os
from typing import List
from langchain.callbacks.base import BaseCallbackHandler
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import CacheBackedEmbeddings, OpenAIEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema import Document
from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
import streamlit as st

st.set_page_config(page_title="DocumentGPT", page_icon="ğŸ“ƒ")

class ChatCallbackHandler(BaseCallbackHandler):
    message = ""

    def on_llm_start(self, *args, **kwargs):
        self.message_box = st.empty()

    def on_llm_end(self, *args, **kwargs):
        save_message(self.message, "ai")

    def on_llm_new_token(self, token: str, *args, **kwargs):
        self.message += token
        self.message_box.markdown(self.message)

llm = ChatOpenAI(
    temperature=0.1,
    streaming=True,
    callbacks=[
        ChatCallbackHandler(),
    ],
)

@st.cache_data(show_spinner=True)
def embed_file(file):
    file_content = file.read()
    if not os.path.exists("./.cache/files"):
        os.makedirs("./.cache/files")
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")
    text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
        separator="\n",
        chunk_size=600,
        chunk_overlap=100,
    )
    loader = UnstructuredFileLoader(file_path)
    documents = loader.load_and_split(text_splitter=text_splitter)
    embeddings = OpenAIEmbeddings()
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
        embeddings,
        cache_dir,
    )
    vectorstore = FAISS.from_documents(
        documents=documents,
        embedding=cached_embeddings,
    )
    retriever = vectorstore.as_retriever()
    return retriever

def save_message(message, role):
    st.session_state["messages"].append({"message": message, "role": role})

def send_message(message, role, save=True):
    with st.chat_message(role):
        st.markdown(message)
    if save:
        save_message(message, role)

def paint_history():
    for message in st.session_state["messages"]:
        send_message(
            message["message"],
            message["role"],
            save=False,
        )

def format_docs(docs: List[Document]):
    return "\n\n".join(doc.page_content for doc in docs)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
Answer the question using ONLY the following context.
If you don't know the answer just say you don't know.
DON'T make anyting up.

Context: {context}
         """,
        ),
        ("human", "{question}"),
    ]
)

st.title("DocumentGPT")

st.markdown(
    """
Welcom!

Use the chatbot to ask questions to an AI about your files!

Upload your files on the sidebar.
"""
)

with st.sidebar:
    file = st.file_uploader(
        label="Upload a file(.txt, .pdf, .docs)",
        type=["pdf", "txt", "docx"],
    )

if file:
    retriever = embed_file(file)
    send_message("I'm ready Ask away!", "ai", save=False)
    paint_history()
    message = st.chat_input("Ask anything about this file")
    if message:
        send_message(message, "human")
        chain = (
            {
                "context": retriever | RunnableLambda(format_docs),
                "question": RunnablePassthrough(),
            }
            | prompt
            | llm
        )
        with st.chat_message("ai"):
            response = chain.invoke(message)

else:
    st.session_state["messages"] = []
```

## HuggingFace

Huggingfaceì—ì„œ ë‹¤ì–‘í•œ ìœ ë£Œ/ë¬´ë£Œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤

[Mistral Model](https://huggingface.co/mistralai/Mistral-7B-v0.1), [API](https://huggingface.co/docs/api-inference/detailed_parameters#text-generation-task), [Docs](https://docs.mistral.ai/)

Instructor modelì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš° ì•„ë˜ì˜ ê°€ì´ë“œ ì°¸ê³ 

The template used to build a prompt for the Instruct model is defined as follows:

`<s>[INST] Instruction [/INST] Model answer</s>[INST] Follow-up instruction [/INST]`

```python
from langchain.llms import HuggingFaceHub
from langchain.prompts import PromptTemplate

prompt = PromptTemplate.from_template(
    "[INST] What is the meaning of {word} [/INST]",
)

llm = HuggingFaceHub(
    repo_id="mistralai/Mistral-7B-Instruct-v0.2",
    model_kwargs={
        "max_new_tokens": 250,
    },
)

chain = prompt | llm

chain.invoke({"word": "flutter"})
```

## PrivateGPT

ë¡œì»¬ì—ì„œ ëª¨ë¸ ì‹¤í–‰

- PromptTemplate
  ë¹„êµì  ì‘ì€ ì‚¬ì´ì¦ˆì˜ `gpt-2` ëª¨ë¸ì„ ì‚¬ìš©
  gpt-2ëŠ” auto completeì— ê°•ì ì´ ìˆë‹¤.

  ```python
  from langchain.llms.huggingface_pipeline import HuggingFacePipeline
  from langchain.prompts import PromptTemplate

  prompt = PromptTemplate.from_template(
      "A {word} is a",
  )

  llm = HuggingFacePipeline.from_model_id(
      model_id="gpt2",
      task="text-generation",
      device=-1,  # 0[GPU], -1[CPU(Default)]
      pipeline_kwargs={
          "max_new_tokens": 150,
      },
  )

  chain = prompt | llm

  chain.invoke({"word": "kimchi"})
  ```

- [GPT4ALL](https://gpt4all.io/index.html)
  - fine tunning ëœ ëª¨ë¸ë“¤ì„ ì§ì ‘ ëª¨ë¸ì„ ë‹¤ìš´ë°›ì•„ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤
- [Ollama](https://ollama.ai/)

  - `ollama run {model}` ì„¤ì¹˜ ë° ì‹¤í–‰
  - ëª¨ë¸ êµì²´ë§Œìœ¼ë¡œ ë™ì¼í•œ promptë¥¼ ì‹¤í–‰ ì‹œí‚¬ ìˆ˜ ìˆë‹¤.
  - `OllamaEmbeddings`, `ChatOllama`

  ```python
  import os
  from typing import List
  from langchain.callbacks.base import BaseCallbackHandler
  from langchain.chat_models import ChatOllama
  from langchain.document_loaders import UnstructuredFileLoader
  from langchain.embeddings import CacheBackedEmbeddings, OllamaEmbeddings
  from langchain.prompts import ChatPromptTemplate
  from langchain.schema import Document
  from langchain.schema.runnable import RunnableLambda, RunnablePassthrough
  from langchain.storage import LocalFileStore
  from langchain.text_splitter import CharacterTextSplitter
  from langchain.vectorstores.faiss import FAISS
  import streamlit as st

  st.set_page_config(page_title="PrivateGPT", page_icon="ğŸ”")

  class ChatCallbackHandler(BaseCallbackHandler):
      message = ""

      def on_llm_start(self, *args, **kwargs):
          self.message_box = st.empty()

      def on_llm_end(self, *args, **kwargs):
          save_message(self.message, "ai")

      def on_llm_new_token(self, token: str, *args, **kwargs):
          self.message += token
          self.message_box.markdown(self.message)

  llm = ChatOllama(
      model="mistral:latest",
      temperature=0.1,
      callbacks=[
          ChatCallbackHandler(),
      ],
  )

  @st.cache_data(show_spinner=True)
  def embed_file(file):
      file_content = file.read()
      if not os.path.exists("./.cache/private_files"):
          os.makedirs("./.cache/private_files")
      file_path = f"./.cache/private_files/{file.name}"
      with open(file_path, "wb") as f:
          f.write(file_content)

      if not os.path.exists("./.cache/private_embeddings"):
          os.makedirs("./.cache/private_embeddings")
      cache_dir = LocalFileStore(f"./.cache/private_embeddings/{file.name}")
      text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
          separator="\n",
          chunk_size=600,
          chunk_overlap=100,
      )
      loader = UnstructuredFileLoader(file_path)
      documents = loader.load_and_split(text_splitter=text_splitter)
      embeddings = OllamaEmbeddings(model="mistral:latest")
      cached_embeddings = CacheBackedEmbeddings.from_bytes_store(
          embeddings,
          cache_dir,
      )
      vectorstore = FAISS.from_documents(
          documents=documents,
          embedding=cached_embeddings,
      )
      retriever = vectorstore.as_retriever()
      return retriever

  def save_message(message, role):
      st.session_state["messages"].append({"message": message, "role": role})

  def send_message(message, role, save=True):
      with st.chat_message(role):
          st.markdown(message)
      if save:
          save_message(message, role)

  def paint_history():
      for message in st.session_state["messages"]:
          send_message(
              message["message"],
              message["role"],
              save=False,
          )

  def format_docs(docs: List[Document]):
      return "\n\n".join(doc.page_content for doc in docs)

  prompt = ChatPromptTemplate.from_template(
      """
  Answer the question using ONLY the following context and not training data.
  If you don't know the answer just say you don't know.
  DON'T make anyting up.

  Context: {context}
  Question: {question}
           """,
  )

  st.title("DocumentGPT")

  st.markdown(
      """
  Welcom!

  Use the chatbot to ask questions to an AI about your files!

  Upload your files on the sidebar.
  """
  )

  with st.sidebar:
      file = st.file_uploader(
          label="Upload a file(.txt, .pdf, .docs)",
          type=["pdf", "txt", "docx"],
      )

  if file:
      retriever = embed_file(file)
      send_message("I'm ready Ask away!", "ai", save=False)
      paint_history()
      message = st.chat_input("Ask anything about this file")
      if message:
          send_message(message, "human")
          chain = (
              {
                  "context": retriever | RunnableLambda(format_docs),
                  "question": RunnablePassthrough(),
              }
              | prompt
              | llm
          )
          with st.chat_message("ai"):
              response = chain.invoke(message)

  else:
      st.session_state["messages"] = []
  ```

## QuizGPT

- ì£¼ìš” í¬ì¸íŠ¸: `Output Parser`, `Function Calling`
- [Wikipedia Api](https://python.langchain.com/docs/integrations/retrievers/wikipedia)
  ```python
  topic = st.text_input(label="Search Wikipedia...")
          if topic:
              retriever = WikipediaRetriever(top_k_results=5)  # type: ignore
              with st.status("Searching wikipedia..."):
                  docs = retriever.get_relevant_documents(topic)
                  st.write(docs)
  ```
- quizë¥¼ ìƒì„±í•˜ëŠ” prompt â†’ formatting prompt â†’ output parser (json) â†’ form ui

  ````python
  import os
  import json
  from typing import List
  from langchain.callbacks import StreamingStdOutCallbackHandler
  from langchain.chat_models import ChatOpenAI
  from langchain.document_loaders import UnstructuredFileLoader
  from langchain.prompts import ChatPromptTemplate
  from langchain.retrievers import WikipediaRetriever
  from langchain.schema import BaseOutputParser, Document
  from langchain.text_splitter import CharacterTextSplitter
  import streamlit as st

  def format_docs(docs: List[Document]):
      return "\n\n".join(doc.page_content for doc in docs)

  @st.cache_data(show_spinner="Loading file...")
  def split_file(file):
      file_content = file.read()
      if not os.path.exists("./.cache/quiz_files"):
          os.makedirs("./.cache/quiz_files")
      file_path = f"./.cache/quiz_files/{file.name}"
      with open(file_path, "wb") as f:
          f.write(file_content)

      text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
          separator="\n",
          chunk_size=600,
          chunk_overlap=100,
      )
      loader = UnstructuredFileLoader(file_path)
      documents = loader.load_and_split(text_splitter=text_splitter)
      return documents

  @st.cache_data(show_spinner="Making quiz...")
  # docsë¥¼ hashingí•  ìˆ˜ ì—†ê¸° ë–„ë¬¸ì— topicìœ¼ë¡œ êµ¬ë¶„
  def run_quiz_chain(_docs, topic):
      chain = {"context": questions_chain} | formatting_chain | output_parser
      return chain.invoke(_docs)

  @st.cache_data(show_spinner="Searching wikipedia...")
  def wiki_search(term):
      retriever = WikipediaRetriever(
          top_k_results=5,
          # lang="ko",
      )  # type: ignore
      return retriever.get_relevant_documents(term)

  class JsonOutputParser(BaseOutputParser):
      def parse(self, text):
          text = text.replace("```json", "").replace("```", "")
          return json.loads(text)

  llm = ChatOpenAI(
      temperature=0.1,
      model="gpt-3.5-turbo-1106",
      streaming=True,
      callbacks=[StreamingStdOutCallbackHandler()],
  )

  questions_prompt = ChatPromptTemplate.from_messages(
      [
          (
              "system",
              """
  You are a helpful assistant that is role playing as a teacher.
  Based ONLY on the following context make 10 questions to test the user's knowledge about the text.
  Each question should have 4 answers, three of them must be incorrect and one should be correct.
  Use (o) to signal the correct answer.

  Question examples:

  Question: What is the color of the ocean?
  Answers: Red|Yellow|Green|Blue(o)

  Question: What is the capital or Georgia?
  Answers: Baku|Tbilisi(o)|Manila|Beirut

  Question: When was Avatar released?
  Answers: 2007|2001|2009(o)|1998

  Question: Who was Julius Caesar?
  Answers: A Roman Emperor(o)|Painter|Actor|Model

  Your turn!
  Make 10 different questions

  Context: {context}
  """,
          )
      ]
  )

  questions_chain = {"context": format_docs} | questions_prompt | llm

  formatting_prompt = ChatPromptTemplate.from_messages(
      [
          (
              "system",
              """
      You are a powerful formatting algorithm.

      You format exam questions into JSON format.
      Answers with (o) are the correct ones.

      Example Input:

      Question: What is the color of the ocean?
      Answers: Red|Yellow|Green|Blue(o)

      Question: What is the capital or Georgia?
      Answers: Baku|Tbilisi(o)|Manila|Beirut

      Question: When was Avatar released?
      Answers: 2007|2001|2009(o)|1998

      Question: Who was Julius Caesar?
      Answers: A Roman Emperor(o)|Painter|Actor|Model

      Example Output:

      ```json
      {{ "questions": [
              {{
                  "question": "What is the color of the ocean?",
                  "answers": [
                          {{
                              "answer": "Red",
                              "correct": false
                          }},
                          {{
                              "answer": "Yellow",
                              "correct": false
                          }},
                          {{
                              "answer": "Green",
                              "correct": false
                          }},
                          {{
                              "answer": "Blue",
                              "correct": true
                          }},
                  ]
              }},
                          {{
                  "question": "What is the capital or Georgia?",
                  "answers": [
                          {{
                              "answer": "Baku",
                              "correct": false
                          }},
                          {{
                              "answer": "Tbilisi",
                              "correct": true
                          }},
                          {{
                              "answer": "Manila",
                              "correct": false
                          }},
                          {{
                              "answer": "Beirut",
                              "correct": false
                          }},
                  ]
              }},
                          {{
                  "question": "When was Avatar released?",
                  "answers": [
                          {{
                              "answer": "2007",
                              "correct": false
                          }},
                          {{
                              "answer": "2001",
                              "correct": false
                          }},
                          {{
                              "answer": "2009",
                              "correct": true
                          }},
                          {{
                              "answer": "1998",
                              "correct": false
                          }},
                  ]
              }},
              {{
                  "question": "Who was Julius Caesar?",
                  "answers": [
                          {{
                              "answer": "A Roman Emperor",
                              "correct": true
                          }},
                          {{
                              "answer": "Painter",
                              "correct": false
                          }},
                          {{
                              "answer": "Actor",
                              "correct": false
                          }},
                          {{
                              "answer": "Model",
                              "correct": false
                          }},
                  ]
              }}
          ]
       }}
      ```
      Your turn!

      Questions: {context}

  """,
          )
      ]
  )

  formatting_chain = formatting_prompt | llm

  output_parser = JsonOutputParser()

  ################################################################################

  st.set_page_config(page_title="QuizGPT", page_icon="â“")

  st.title("QuizGPT")

  with st.sidebar:
      topic = None
      docs = None
      choice = st.selectbox(
          "Choose what you want to use",
          (
              "File",
              "Wikipedia Article",
          ),
      )

      if choice == "File":
          file = st.file_uploader(
              label="Upload a file(.txt, .pdf, .docs)",
              type=["pdf", "txt", "docx"],
          )
          if file:
              docs = split_file(file)

      else:
          topic = st.text_input(label="Search Wikipedia...")
          if topic:
              docs = wiki_search(topic)

  if not docs:
      st.markdown(
          """
  Welcom to QuizGPT.

  I will make a quiz from Wikipedia articles or files you upload to test your knowledge and help you study.

  Get started by uploading a file or searching on Wikipedia in the sidebar.

  """
      )
  else:
      response = run_quiz_chain(
          docs,
          topic if topic else file.name,  # type: ignore
      )
      st.write(response)
      with st.form(key="questions_form"):
          for question in response["questions"]:
              st.write(question["question"])
              value = st.radio(
                  key=question["question"],
                  label="Select correct answer",
                  options=[answer["answer"] for answer in question["answers"]],
                  index=None,
              )
              if {"answer": value, "correct": True} in question["answers"]:
                  st.success("âœ…")
              elif value is not None:
                  correct = list(
                      filter(lambda answer: answer["correct"], question["answers"])
                  )[0]["answer"]
                  st.error(f"âŒ: {correct}")

          button = st.form_submit_button()
  ````

- function calling (gpt)

  - functionë“¤ì„ ì œê³µí•˜ê³  í•„ìš”ì— ë”°ë¼ ì‚¬ìš©í•˜ë„ë¡
  - ì˜ˆì‹œ) ë‚ ì”¨ ì •ë³´ì™€ ê°™ì€ ì‹¤ì‹œê°„ ì •ë³´ëŠ” í•¨ìˆ˜ë¥¼ í†µí•´ ì‹¤ì‹œê°„ ì •ë³´ë¥¼ ë¶ˆëŸ¬ì™€ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤

  ```python
  import json
  from langchain.chat_models import ChatOpenAI
  from langchain.prompts import PromptTemplate

  def get_weather(longitude, latitude):
      print(f"weather for longitude({longitude}) and latitude({latitude})")

  function = {
      "name": "get_weather",
      "description": """
      function that takes longitude and latitude to find the weather of a place
      """,
      "parameters": {
          "type": "object",
          "properties": {
              "longitude": {
                  "type": "string",
                  "description": "The longitude coordinate",
              },
              "latitude": {
                  "type": "string",
                  "description": "The latitude coordinate",
              },
          },
      },
      "required": ["longitude", "latitude"],
  }

  llm = ChatOpenAI(temperature=0.1).bind(
      function_call="auto",
      functions=[function],
  )

  prompt = PromptTemplate.from_template("Who is the weather in {city}")

  chain = prompt | llm

  response = chain.invoke({"city": "rome"})

  response = response.additional_kwargs["function_call"]["arguments"]

  response = json.loads(response)

  get_weather(response["longitude"], response["latitude"])
  ```

## SiteGPT

TODOS

- [ ] Streaming
- [ ] Memory chat history
- [ ] Cache (similar) question
  - [ ] question listì—ì„œ ìœ ì‚¬í•œ ì§ˆë¬¸ì„ ì°¾ëŠ”ë‹¤
- AsyncChromiumLoader, Html2TextTransformer
- siteë¥¼ loadí•˜ê³  html ë¬¸ì„œë¥¼ textë¡œ ë³€í™˜í•˜ëŠ” ê³¼ì •

```python
from langchain.document_loaders import AsyncChromiumLoader
from langchain.document_transformers import Html2TextTransformer
import streamlit as st

st.set_page_config(page_title="SiteGPT", page_icon="ğŸ“Š")

html2text_transformer = Html2TextTransformer()

################################################################
st.title("SiteGPT")

st.markdown(
    """
# SiteGPT

Ask questions about the content of a website.

Start by writing the URL of the website on the sidebar.
"""
)

with st.sidebar:
    url = st.text_input(
        "Write down a URL",
        placeholder="https://example.com",
    )

if url:
    loader = AsyncChromiumLoader(urls=[url])
    docs = loader.load()
    transformed = html2text_transformer.transform_documents(docs)
    st.write(transformed)
```

- sitemap loader
  - blogê´€ë ¨ urlë§Œ filter(regex)
  - parsing_functionì„ í†µí•´ ë¶ˆí•„ìš”í•œ header, footer ë“±ì± ìš”ì†Œ ì œê±°
  - text_splitter

```python
def parse_page(soup):
    """
    headerì™€ footerë¥¼ ì œê±°í•œ contents ë°˜í™˜
    """
    header = soup.find("header")
    footer = soup.find("footer")
    if header:
        header.decompose()
    if footer:
        footer.decompose()
    return (
        str(soup.get_text())
        .replace("\n", " ")  # ì¤„ë°”ê¿ˆ
        .replace("\xa0", " ")  # ê³µë°± ë¬¸ì
        .replace("CloseSearch Submit Blog", "")
    )

@st.cache_data(show_spinner="Loading website...")
def load_website(url):
    text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(
        chunk_size=1000,
        chunk_overlap=200,
    )

    loader = SitemapLoader(
        url,
        filter_urls=[r"^(.*\/blog\/).*"],
        parsing_function=parse_page,
    )
    loader.requests_per_second = 5  # ë„ˆë¬´ ë¹ ë¥´ë©´ ì°¨ë‹¨ ë‹¹í•  ìˆ˜ ìˆë‹¤ (default 2)
    docs = loader.load_and_split(text_splitter=text_splitter)
    return docs
```

- Map Re Rank
  - ë‹µë³€ì„ ì ìˆ˜ì™€ í•¨ê»˜ ì œê³µí•˜ë„ë¡

```python
answers_prompt = ChatPromptTemplate.from_template(
    """
    Using ONLY the following context answer the user's question.
    If you can't just say you don't know, don't make anything up.

    Then, give a score to the answer between 0 and 5.
    The score should be high if the answer is related to the user's question, and low otherwise.
    If there is no relevant content, the score is 0.
    Always provide scores with your answers
    Context: {context}

    Examples:

    Question: How far away is the moon?
    Answer: The moon is 384,400 km away.
    Score: 5

    Question: How far away is the sun?
    Answer: I don't know
    Score: 0

    Your turn!
    Question: {question}
"""
)

def get_answers(inputs):
    docs = inputs["docs"]
    question = inputs["question"]
    answers_chain = answers_prompt | llm
    return {
        "question": question,
        "answers": [
            {
                "answer": answers_chain.invoke(
                    {"context": doc.page_content, "question": question}
                ).content,
                "source": doc.metadata["source"],
                "date": doc.metadata["lastmod"],
            }
            for doc in docs
        ],
    }

choose_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            """
            Use ONLY the following pre-existing answers to answer the user's question.
            Use the answers that have the highest score (more helpful) and favor the most recent ones.
            Cite sources and return the sources of the answers as they are with Date, do not change them.
            Answers: {answers}
            """,
        ),
        ("human", "{question}"),
    ]
)

def choose_answer(inputs):
    answers = inputs["answers"]
    question = inputs["question"]
    choose_chain = choose_prompt | llm
    condensed = "\n\n".join(
        f'{answer["answer"]}\nSource:{answer["source"]}\nDate{answer["date"]}\n'
        for answer in answers
    )

    return choose_chain.invoke(
        {"question": question, "answers": condensed},
    )
```

## MeetingGPT

ì˜ìƒ ìš”ì•½

ë¹„ë””ì˜¤ â†’ ì˜¤ë””ì˜¤ ì¶”ì¶œ(ffmpeg) â†’ 10ë¶„ ë‹¨ìœ„ chunk(split) â†’ speech to text (whisper) â†’ summarize whole content â†’ embed

- ì˜¤ë””ë„ ì¶”ì¶œ (ffmpeg) -vn: ignore video
  `ffmpeg -i input_path -vn output_path`
  subprocess ë¡œ clië¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆë‹¤
  ```python
  def extract_audio_from_video(video_path, audio_path):
      command = ["ffmpeg", "-i", video_path, "-vn", audio_path]
      subprocess.run(command)
  ```
- split audio (pydub)

```python
def split_audio(audio_path, chunks_folder, chunk_size=10):
    track = AudioSegment.from_mp3(audio_path)
    chunk_len = chunk_size * 60 * 1000
    chunks = math.ceil(len(track) / chunk_len)

    for i in range(chunks):
        start_time = i * chunk_len
        end_time = (i + 1) * chunk_len
        chunk = track[start_time:end_time]
        chunk.export(f"{chunks_folder}/chunk_{str(i).zfill(2)}.mp3", format="mp3")
```

- transcribe

```python
import openai
from glob import glob

def transcribe_chunks(chunk_folder, destination):
    files = sorted(glob(f"{chunk_folder}/*.mp3"))

    for file in files:
        with open(file, "rb") as audio_file, open(destination, "a") as text_file:
            transcript = openai.Audio.transcribe("whisper-1", audio_file)
            text_file.write(transcript["text"])

transcribe_chunks("./files/audios/chunks", "./files/audios/transcript.txt")
```
